这是为您整理的完整参考文献列表。您可以直接复制以下内容到 Word 文档的“参考文献”部分。

为了符合学术规范，该列表整合了综述、原理分析和防御架构章节中引用的所有核心文献，并已统一格式。

---

### 参考文献 (References)

**[1]** OWASP Foundation. *OWASP Top 10 for Large Language Model Applications*. 2025. [Online]. Available: [https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/](https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/)

**[2]** Greshake K, Abdelnabi S, Mishra S, et al. Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection[C]//Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security. 2023: 79-90.

**[3]** Zou A, Wang Z, Kolter J Z, et al. Universal and Transferable Adversarial Attacks on Aligned Language Models[J]. arXiv preprint arXiv:2307.15043, 2023.

**[4]** Liu Y, Deng G, Li Y, et al. Prompt Injection Attack against LLM-integrated Applications[J]. arXiv preprint arXiv:2306.05499, 2023.

**[5]** Clusmann J, Ferber D, Wiest I C, et al. Prompt injection attacks on vision language models in oncology[J]. Nature Communications, 2024, 15(1): 1239.

**[6]** Debenedetti E, Zhang J, Balunovic M, et al. AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents[C]//NeurIPS, 2024.

**[7]** Wang Z, et al. Manipulating Multimodal Agents via Cross-Modal Prompt Injection[J]. arXiv preprint arXiv:2504.14348, 2025.

**[8]** Ferrag M A, Tihanyi N, Hamouda D, et al. From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows[J]. arXiv preprint arXiv:2506.23260, 2025.

**[9]** Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008.

**[10]** Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.

**[11]** Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.

**[12]** Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744.

**[13]** HuggingFace. *Transformers Library: modeling_llama.py*. GitHub Repository. [Online]. Available: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

**[14]** Mo Y, et al. Fight Back Against Jailbreaking Via Prompt Adversarial Tuning[C]//Conference on Neural Information Processing Systems. 2024.

**[15]** Yang H. TAPDA: Text Adversarial Purification as Defense Against Adversarial Prompt Attack for Large Language Models[C]//2025 8th International Conference on Advanced Algorithms and Control Engineering (ICAACE). IEEE, 2025: 1998-2004.

**[16]** Lin H, et al. UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models[J]. arXiv preprint arXiv:2502.13141, 2025.

**[17]** Kumar A, et al. Certifying LLM Safety Against Adversarial Prompting[C]//International Conference on Learning Representations (ICLR). 2024.

**[18]** Emekci H, Budakoglu G. Securing with Dual-LLM Architecture: ChatTEDU an Open Access Chatbot’s Defense[J]. ISSS Journal of Micro and Smart Systems, 2025: 1-1.